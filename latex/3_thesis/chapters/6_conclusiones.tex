%% ------------------------------------------------------------------------- %%
\chapter{Conclusiones}
\label{cap:conclusiones}
\lhead{\emph{Conclusiones}}  


%\section{Conclusiones}

\textbf{PRIMERA}: Se realizó una revisión sistemática de la literatura respecto a los métodos que utilizan Transformers para la predicción del enlace pMHC. Este análisis reforzó la hipótesis del uso de Transformers y \textit{transfer learning} en los trabajos  recientes, estos motivados por el gran auge de los modelos BERT en el procesamiento natural de lenguaje. Adicionalmente a esto, también se incluyó los métodos adyacentes necesarios para la detección de neoantígenos en el marco de desarrollo de vacunas personalizadas contra el cáncer. Estos métodos incluyen el análisis de métodos para la predicción de la unión pMHC-TCR, \textit{pipelines} y ensayos clínicos. Este análisis ha demostrado que existen varios problemas en el proceso de detección de neoantígenos como: la no inclusión de datos de MS, bajo rendimiento en las herramientas de predicción pMHC y PMHC-TCR, así como la falta de métodos para la detección de fusión de genes y \textit{alternative splicing}; sin embargo, a pesar de estas limitaciones, ya se han realizado ensayos clínicos con resultados motivadores.

\textbf{SEGUNDA}: Se analizó los modelos Transformers pre-entrenados como: TAPE, ProtBert-BFD, y la familia de modelos de EMS2. Estos modelos fueron entrenados en grandes bases de datos de proteínas para tareas como: Predicción de estructuras de proteínas, predicción del \textit{contact map}, predicción de la función de proteínas, etc. De estos, ProtBert-BFD fue entrenado con la base de datos mas grande proteínas actualmente (2122 millones de muestras); sin embargo, las muestras tienen ruido y  el modelo presentó resultados pobres en los experimentos de esta tésis. Adicionalmente, TAPE se entreno con la base de datos mas pequeña (30 millones de muestras); sin embargo, presentó mejores resultados en los experimentos porque las muestras pertenecían a \textit{Reference Proteomas}, y esto generó que el modelo represente mejor la información de una proteína.

\textbf{TERCERA}: Se aplicó \textit{fine-tuning} a los seis modelos pre-entenados agregando un bloque BilSTM. Adicionalmente, se evaluó el uso de \textit{Gradient Accumulation Steps} (GAS) y una metodología para congelar las capas del modelo Transformer. Como resultado, se verificó que en los modelos ESM2, al aplicar el congelamiento de capas, se mejoró el desempeño y además el modelo mas pequeño EMS2(t6), obtuvo los mejores resultados. Esto puede ser causado por el uso RoPE y al tener una base datos aún pequeña para la complejidad de estos datos. ProtBert, era uno de los modelos mas grandes y obtuvo un desempeño pobre, esto porque fue entrenado con una base de datos con mucho ruido. Finalmente, TAPE obtuvo los mejores resultados, el cúal se debe a la calidad de las muestras utilizadas en su pre-entrenamiento.


Adicionalmente, en el análisis comparativo de los seis modelos Transformers: TAPE, ProtBert-BFD, ESM2(t6), ESM2(t12), ESM2(t30) y ESM2(t33) con la incorporación de GAS y la técnica de congelación de capas. Observamos que ESM2(t6)-Freeze y TAPE-GAS lograron los resultados más favorables. Además, observamos que el uso de GAS ofreció una mitigación menor del problema de \textit{vanish gradientes}, lo que permitió el entrenamiento efectivo de modelos Transformer más grandes. Además, descubrimos que la metodología de congelación de capas aceleró el proceso de entrenamiento y produjo los resultados más favorables para los modelos ESM2. En contraste, el uso de GAS condujo a los mejores resultados para TAPE y ProtBert.

\textbf{CUARTA}: Tambien, se volvio a entrenar los modelos de mejor desempeño ESM2(t6)-Freeze y TAPE-GAS, esta vez por 30 \textit{epochs}, para mejorar aún más su desempeño y luego se realizó una comparación con los métodos de mejor desempeño en el estado del arte, tales como: NetMHCpan4.1, MHCflurry2.0, Anthem, ACME y MixMHCpred2.2. Se realizó la comparación en términos de diversas métricas de rendimiento como el AUC, \textit{accuracy, presicion, recall, f1-score} y MCC. De esta comparativa, los modelos propuestos ESM2(t6)-Freeze y TAPE-GAS superaron a los demás métodos del estado del arte en \textit{AUC, accuracy, recall, f1-score} y MCC. Esto demuestra las ventajas de aplicar \textit{fine-tuning} a modelos Transformer grandes para predecir la unión péptido-MHC.






\section{Trabajos Futuros}
En este trabajo, evaluamos los modelos TAPE, ProtBert-BFD, ESM2(t6), ESM2(t12), ESM2(t30) y ESM2(t33), cada uno con 92, 420, 8, 35, 150 y 650 millones de parámetros respectivamente. Sin embargo, existen otras alternativas como ProtT5-XL y ProtT5-XXL, ESM2(t36) y ESM2(t48), cada uno con 3, 11, 3 y 15 billones de parámetros respectivamente. No evaluamos estos modelos debido al tamaño reducido del conjunto de datos y el costo de entrenamiento. No obstante, planeamos evaluar estos enormes modelos Transformer con un conjunto de datos más grande que contenga muestras del conjunto de datos Anthem, MixMHXpred2.2 y la evaluación más reciente de herramientas de predicción de unión pMHC.

Además, dada la considerable inversión de recursos asociada al entrenamiento de modelos Transformer grandes, planeamos investigar las posibles ventajas de utilizar DistilBERT y LoRA para tareas de entrenamiento y predicciones.

Además, en este trabajo, se aplico \textit{fine-tuning} al modelo Transformer agregando un bloque BiLSTM al final, basado en el trabajo de HLAB. En el futuro, planeamos evaluar la eficacia de un bloque Star-Transformer, similar a la metodología empleada en SMHCpan. Además, considerando los resultados prometedores demostrados en ESM-GAT, creemos que la inclusión de una Red de Atención de Grafos (GAT) podría mejorar significativamente el rendimiento de nuestro modelo en investigaciones futuras. Por último, nos gustaría evaluar la metodología utilizada por TransPHLA, debido a su efectividad demostrada en el manejo de péptidos de diferentes longitudes.