%% ------------------------------------------------------------------------- %%
\chapter{Conclusiones y Trabajos Futuros}
\label{cap:conclusiones}
\lhead{\emph{Conclusiones}}  


\section{Conclusiones}

\textbf{PRIMERA}: Se revisó y analizó los métodos que utilizan \textit{Transformers} para la predicción del enlace pMHC. Este análisis reforzó la hipótesis del uso de \textit{Transformers} y \textit{transfer learning} en los trabajos  recientes, estos motivados por el gran auge de los modelos BERT en el procesamiento natural de lenguaje. Adicionalmente a esto, también se incluyó los métodos adyacentes necesarios para la detección de neoantígenos en el marco de desarrollo de vacunas personalizadas contra el cáncer. Estos métodos incluyen el análisis de métodos para la predicción de la unión pMHC-TCR, \textit{pipelines} y ensayos clínicos. Este análisis ha demostrado que existen varios problemas en el proceso de detección de neoantígenos como: la no inclusión de datos de MS, bajo rendimiento en las herramientas de predicción pMHC y PMHC-TCR, así como la falta de métodos para la detección de fusión de genes y \textit{alternative splicing}; sin embargo, a pesar de estas limitaciones, ya se han realizado ensayos clínicos con resultados motivadores.

\textbf{SEGUNDA}: Se analizó los modelos \textit{Transformers} pre-entrenados como: TAPE, ProtBert-BFD, y la familia de modelos de EMS2. Estos modelos fueron entrenados en grandes bases de datos de proteínas para tareas como: Predicción de estructuras de proteínas, predicción del \textit{contact map}, predicción de la función de proteínas, etc. De estos, ProtBert-BFD fue entrenado con la base de datos mas grande proteínas actualmente (2122 millones de muestras); sin embargo, las muestras tienen ruido y  el modelo presentó resultados pobres en los experimentos de esta tesis. Adicionalmente, TAPE se entreno con la base de datos mas pequeña (30 millones de muestras); sin embargo, presentó mejores resultados en los experimentos porque las muestras pertenecían a \textit{Reference Proteomas}, y esto generó que el modelo represente mejor la información de una proteína.

\textbf{TERCERA}: Se aplicó \textit{fine-tuning} a los seis modelos pre-entrenados agregando un bloque BiLSTM. Adicionalmente, se evaluó el uso de \textit{Gradient Accumulation Steps} (GAS) y una metodología para congelar las capas del modelo \textit{Transformer}. Como resultado, se verificó que en los modelos ESM2, al aplicar el congelamiento de capas, se mejoró el desempeño y además el modelo mas pequeño EMS2(t6), obtuvo los mejores resultados. Esto puede ser causado por el uso RoPE y al tener una base datos aún pequeña para la complejidad de estos datos. ProtBert, era uno de los modelos mas grandes y obtuvo un desempeño pobre, esto porque fue entrenado con una base de datos con mucho ruido. Finalmente, TAPE obtuvo los mejores resultados, el cual se debe a la calidad de las muestras utilizadas en su pre-entrenamiento.


Adicionalmente, en el análisis comparativo de los seis modelos \textit{Transformers}: TAPE, ProtBert-BFD, ESM2(t6), ESM2(t12), ESM2(t30) y ESM2(t33) con la incorporación de GAS y la técnica de congelación de capas. Observamos que ESM2(t6)-Freeze y TAPE-GAS lograron los resultados más favorables. Además, observamos que el uso de GAS ofreció una mitigación menor del problema de \textit{vanish gradientes}, lo que permitió el entrenamiento efectivo de modelos \textit{Transformer} más grandes. Además, descubrimos que la metodología de congelación de capas aceleró el proceso de entrenamiento y produjo los resultados más favorables para los modelos ESM2. En contraste, el uso de GAS condujo a los mejores resultados para TAPE y ProtBert.

\textbf{CUARTA}: También, se volvió a entrenar los modelos de mejor desempeño ESM2(t6)-Freeze y TAPE-GAS, esta vez por 30 \textit{epochs}, para mejorar aún más su desempeño y luego se realizó una comparación con los métodos de mejor desempeño en el estado del arte, tales como: NetMHCpan4.1, MHCflurry2.0, Anthem, ACME y MixMHCpred2.2. Se realizó la comparación en términos de diversas métricas de rendimiento como el AUC, \textit{accuracy, precision, recall, f1-score} y MCC. De esta comparativa, los modelos propuestos ESM2(t6)-Freeze y TAPE-GAS superaron a los demás métodos del estado del arte en \textit{AUC, accuracy, recall, f1-score} y MCC. Esto demuestra las ventajas de aplicar \textit{fine-tuning} a modelos \textit{Transformer} grandes para predecir la unión péptido-MHC.



\textbf{QUINTA}: Finalmente, se ha logrado cumplir con el objetivo general de la tesis. Se ha implementado un método  \textit{in silico} basado en \textit{Transformers} y \textit{Transfer Learning} para la detección de neoantígenos, enfocados en la predicción de la unión pMHC. Para llevar a cabo esta implementación, se evaluó varios modelos \textit{Transformers} pre-entrenados y se realizó \textit{Transfer Learning} aplicando \textit{fine-tuning} a los modelos \textit{Transformer}. Finalmente, se comparó estos modelos con las herramientas de mejor desempeño en el estado del arte. Luego de las comparaciones, el método implementado obtuvo el mejor desempeño en términos de AUC, \textit{accuracy, recall, f1-score} y MCC.


\section{Trabajos Futuros}
En este trabajo, evaluamos los modelos TAPE, ProtBert-BFD, ESM2(t6), ESM2(t12), ESM2(t30) y ESM2(t33), cada uno con 92, 420, 8, 35, 150 y 650 millones de parámetros respectivamente. Sin embargo, existen otras alternativas como ProtT5-XL y ProtT5-XXL, ESM2(t36) y ESM2(t48), cada uno con 3, 11, 3 y 15 billones de parámetros respectivamente. No evaluamos estos modelos debido al tamaño reducido del conjunto de datos y el costo de entrenamiento. No obstante, planeamos evaluar estos enormes modelos \textit{Transformer} con un conjunto de datos más grande que contenga muestras del conjunto de datos Anthem, MixMHCpred2.2 y la evaluación más reciente de herramientas de predicción de unión pMHC.

Además, dada la considerable inversión de recursos asociada al entrenamiento de modelos \textit{Transformer} grandes, planeamos investigar las posibles ventajas de utilizar DistilBERT y LoRA para tareas de entrenamiento y predicciones.

Además, en este trabajo, se aplico \textit{fine-tuning} al modelo \textit{Transformer} agregando un bloque BiLSTM al final, basado en el trabajo de HLAB. En el futuro, planeamos evaluar la eficacia de un bloque Star-\textit{Transformer}, similar a la metodología empleada en SMHCpan. Además, considerando los resultados prometedores demostrados en ESM-GAT, creemos que la inclusión de una Red de Atención de Grafos (GAT) podría mejorar significativamente el rendimiento de nuestro modelo en investigaciones futuras. Por último, nos gustaría evaluar la metodología utilizada por TransPHLA, debido a su efectividad demostrada en el manejo de péptidos de diferentes longitudes.


\section{Agradecimiento}

Esta investigación fue respaldada parcialmente por la Universidad La Salle y la Universidad Católica San Pablo, con código de proyecto: 01-CPICI-2021. Este fue un financiamiento conjunto para promover la investigación entre ambas universidades. 

También, agradezco a mi asesor Dr. Cristian Lopez por su ayuda técnica y soporte en la metodología de la tesis. Adicionalmente, agradezco a los miembros del jurado, por sus comentarios y observaciones de la tesis, estos han sido tomados en cuenta y han mejorado la calidad de la tesis.








