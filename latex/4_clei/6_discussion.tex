\section{Discussion}

%\subsection{Fine-tuning ESM2 models}
When specifically considering ESM2 Transformer models, the most favorable results were obtained with the smallest model, ESM2(t6), as indicated in Table \ref{tab:comparison_3_epochs}. However, it's noteworthy that the authors of ESM2 reported in their paper that for various other tasks, larger models like ESM2(t30) and ESM2(t33) outperformed the smaller ones such as ESM2(t6) and ESM2(t12) \cite{lin2023evolutionary}. Additionally, it is well-established that larger models tend to exhibit faster learning but require more extensive training datasets \cite{elnaggar2021prottrans}. In the case of pMHC-I binding prediction, our study employed a dataset comprising 559,019 samples, which we believe is not sufficiently large for ESM2(t33), a model boasting 650 million parameters. In future research endeavors, we plan to assess the performance of larger models using more extensive datasets. Another potential reason for the superior performance of ESM2(t6) could be attributed to the use of Rotary Position Embedding (RoPE) used instead of absolute positional encoding. While RoPE may lead to a slight increase in training cost, it has been observed to enhance the quality of results, particularly for smaller models \cite{lin2023evolutionary}.

%\subsection{Layer Freezing and GAS}


During the training of Transformer models, we explored the implementation of a layer freezing methodology. This approach involves locking the Transformer model while updating only the BiLSTM parameters. As reported in various studies on freezing methodologies in Transformers \cite{merchant2020happens,lee2019would,kovaleva2019revealing}, this method is generally well-suited to accelerate the training process, even though it may lead to a slight sacrifice in performance. Surprisingly, for ESM2 models, this methodology yielded the best results. Moreover, we encountered a recurring issue of vanishing gradients when training large models such as TAPE-normal, ProtBert-normal, ESM2(t30)-normal, and ESM2(t33)-normal. This challenge is a common occurrence when training large language models, as gradients tend to approach zero values after several training steps. To address this, we reduce the learning rate and increased the warmup steps.


%\subsection{TAPE, ProtBert-BFD and ESM2}
Furthermore, a comparison between TAPE, ProtBert-BFD, and ESM2, each of one described in Table \ref{tab:pretrained}. The metrics are presented in Table \ref{tab:comparison_3_epochs}. According to this information, ProtBert-BFD got the worst result despite the fact this model was pre-trained with the largest dataset BFD with 2122M samples; moreover, it has 420M parameters. We believe this result is caused by the noisy information and sequence mistakes in the BFD dataset \cite{elnaggar2021prottrans}. Moreover, large Transformers models need more data for training \cite{elnaggar2021prottrans}, and we fine-tuned this model with 559019 samples.

Additionally, it is notable that TAPE achieved the best results, with ESM2(t6) following closely (as shown in Table \ref{tab:comparison_3_epochs}). TAPE models were pre-trained using the Pfam dataset, which is the smallest dataset in this comparison, containing approximately 30 million samples. It's important to mention that the Pfam dataset is derived from UniProtKB and selectively includes sequences belonging to Reference Proteomes rather than encompassing the entire UniProtKB database \cite{finn2016pfam}. Consequently, Pfam covers half of the protein sequences compared to other datasets based on UniProtKB, but its samples are of higher quality. Therefore, it is plausible to assume that TAPE encapsulates a more comprehensive and refined representation of protein information compared to other pre-trained models. Moreover, ESM2(t6) achieved results that closely rival TAPE's performance, as demonstrated in Table \ref{tab:comparison}. Notably, ESM2(t6) comprises only 8 million parameters, compared to 92 million parameters of TAPE. Furthermore, both models were trained on samples from UniProtKB, although TAPE used a subset of this dataset. Moreover, ESM2(t6) outperformed TAPE for longer peptides, ranging from 11 to 14 mers, as depicted in Fig. \ref{fig:auc_distribution}. These findings strongly position ESM2(t6) as a prime candidate for future analyses due to its remarkable performance and cost-effectiveness.


\section{Conclusions}

In our comparative analysis of the six Transformer models TAPE, ProtBert-BFD, ESM2(t6), ESM2(t12), ESM2(t30), and ESM2(t33) with the incorporation of GAS and the layer freezing technique, we observed that ESM2(t6)-Freeze, and TAPE-GAS achieved the most favorable outcomes. Additionally, we found that the layer freezing methodology accelerated the training process and produced the most favorable results for ESM2 models. In contrast, using GAS led to the best results for TAPE and ProtBert.

Moreover, after training ESM2(t6)-Freeze and TAPE-GAS for thirty epochs, the models surpassed state-of-the-art methods, including NetMHCpan4.1, MHCflurry2.0, Anthem, ACME, and MixMHCpred2.2, in terms of various performance metrics like AUC, accuracy, recall, f1-score, and MCC. This demonstrates the advantages of fine-tuning large Transformer models for predicting peptide-MHC binding, underscoring their potential to enhance this critical task.

Additionally, the vanishing gradient problem is a common problem when training large BERT models. So, in order to avoid this problem, we noticed that larger models need very small learning rates at the beginning of training. Furthermore, in order to maintain stability, it is very useful to use a scheduler to increase and decrease the learning rate during training. Thus, after experiments, we managed to use a learning rate of $2e-6$ and 200k step for warnup. Additionally, we used the ADAM optimizer with bias correction.

\section{Future works}
In this work, we evaluated TAPE, ProtBert-BFD, ESM2(t6), ESM2(t12), ESM2(t30), and ESM2(t33), each one with 92, 420, 8, 35, 150, and 650 million parameters respectively. However, we have other alternatives like ProtT5-XL and ProtT5-XXL, ESM2(t36), and ESM2(t48), each one with 3, 11, 3, and 15 billion parameters, respectively. We didn't evaluate these models because of the small size of the dataset and the training cost. Nevertheless, we planned to evaluate these huge Transformer models with a large dataset comprising samples from Anthem dataset \cite{mei2021anthem}, MixMHXpred2.2  \cite{gfeller2023improved}, and the most recent benchmarking of pMHC binding prediction tools \cite{wang2023comprehensive}.

Moreover, given the considerable training cost associated with training large Transformer models, we plan to investigate the potential advantages of utilizing DistilBERT \cite{sanh2019distilbert} and LoRA \cite{hu2021lora} for training and future prediction tasks.

Furthermore, we fine-tuned each Transformer model, adding a BiLSTM block at the end, based on the work of HLAB \cite{zhang2022hlab}. Looking ahead, we plan to assess the effectiveness of a Star-Transformer block, similar to the methodology employed in SMHCpan \cite{ye2023stmhcpan}. Furthermore, considering the promising results demonstrated in ESM-GAT \cite{hashemi2023improved}, we believe that the inclusion of a Graph Attention Network (GAT) could significantly enhance our model's performance in future research. Finally, we would like to evaluate the methodology used by TransPHLA \cite{chu2022transformer}, due to its demonstrated effectiveness in handling peptides of different lengths.